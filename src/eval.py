"""
Evaluation Module
√âvalue les performances du mod√®le entra√Æn√©
"""

import pandas as pd
import numpy as np
import os
import joblib
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error


def evaluate_model(data_dir="model", model_path="models/model.pkl"):
    """
    √âvalue le mod√®le entra√Æn√© sur les donn√©es de test
    
    Args:
        data_dir (str): R√©pertoire contenant les donn√©es trait√©es
        model_path (str): Chemin vers le fichier du mod√®le sauvegard√©
        
    Returns:
        dict: Dictionnaire contenant les m√©triques d'√©valuation
    """
    print("\n" + "="*60)
    print("üìà EVALUATION PIPELINE")
    print("="*60 + "\n")
    
    # V√©rifier que le mod√®le existe
    if not os.path.exists(model_path):
        print(f"‚ùå Erreur: Mod√®le {model_path} non trouv√©!")
        return None
    
    # --- √âTAPE 1 : Chargement du mod√®le ---
    print("üîç √âtape 1: Chargement du mod√®le")
    model = load_model(model_path)
    
    if model is None:
        return None
    
    # --- √âTAPE 2 : Chargement des donn√©es de test ---
    print("üìÇ √âtape 2: Chargement des donn√©es de test")
    X_train, X_test, y_train, y_test = load_test_data(data_dir)
    
    if X_test is None:
        print(f"‚ùå Impossible de charger les donn√©es depuis {data_dir}")
        return None
    
    # --- √âTAPE 3 : √âvaluation du mod√®le ---
    print("üìä √âtape 3: √âvaluation du mod√®le\n")
    metrics = evaluate_on_splits(model, X_train, X_test, y_train, y_test)
    
    # --- √âTAPE 4 : Affichage des r√©sultats ---
    print("="*60)
    print("üìã R√âSUM√â DES PERFORMANCES")
    print("="*60)
    print_metrics_summary(metrics)
    print("="*60)
    print("‚úÖ EVALUATION TERMIN√âE")
    print("="*60 + "\n")
    
    return metrics


def load_model(model_path):
    """
    Charge le mod√®le sauvegard√©
    
    Args:
        model_path (str): Chemin vers le fichier du mod√®le
        
    Returns:
        LinearRegression: Mod√®le charg√© ou None en cas d'erreur
    """
    try:
        model = joblib.load(model_path)
        print(f"‚úî Mod√®le charg√©: {model_path}\n")
        return model
    except Exception as e:
        print(f"‚ùå Erreur lors du chargement du mod√®le: {e}\n")
        return None


def load_test_data(data_dir):
    """
    Charge les donn√©es de test et d'entra√Ænement
    
    Args:
        data_dir (str): R√©pertoire contenant les donn√©es
        
    Returns:
        tuple: (X_train, X_test, y_train, y_test) ou (None, None, None, None) en cas d'erreur
    """
    try:
        X_train = pd.read_csv(os.path.join(data_dir, "X_train.csv"))
        X_test = pd.read_csv(os.path.join(data_dir, "X_test.csv"))
        y_train = pd.read_csv(os.path.join(data_dir, "y_train.csv")).iloc[:, 0]
        y_test = pd.read_csv(os.path.join(data_dir, "y_test.csv")).iloc[:, 0]
        
        print(f"‚úî X_train charg√©: {X_train.shape}")
        print(f"‚úî X_test charg√©: {X_test.shape}")
        print(f"‚úî y_train charg√©: {y_train.shape}")
        print(f"‚úî y_test charg√©: {y_test.shape}\n")
        
        return X_train, X_test, y_train, y_test
    
    except FileNotFoundError as e:
        print(f"‚ùå Erreur lors du chargement des donn√©es: {e}\n")
        return None, None, None, None


def evaluate_on_splits(model, X_train, X_test, y_train, y_test):
    """
    √âvalue le mod√®le sur les ensembles d'entra√Ænement et de test
    
    Args:
        model: Mod√®le entra√Æn√©
        X_train, X_test (pd.DataFrame): Features train/test
        y_train, y_test (pd.Series): Target train/test
        
    Returns:
        dict: Dictionnaire contenant toutes les m√©triques
    """
    metrics = {}
    
    # Pr√©dictions sur train
    y_train_pred = model.predict(X_train)
    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
    train_mae = mean_absolute_error(y_train, y_train_pred)
    train_r2 = r2_score(y_train, y_train_pred)
    
    metrics["train"] = {
        "RMSE": train_rmse,
        "MAE": train_mae,
        "R¬≤": train_r2
    }
    
    print("--- TRAIN SET ---")
    print(f"RMSE : {train_rmse:.2f}")
    print(f"MAE  : {train_mae:.2f}")
    print(f"R¬≤   : {train_r2:.4f}\n")
    
    # Pr√©dictions sur test
    y_test_pred = model.predict(X_test)
    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
    test_mae = mean_absolute_error(y_test, y_test_pred)
    test_r2 = r2_score(y_test, y_test_pred)
    
    metrics["test"] = {
        "RMSE": test_rmse,
        "MAE": test_mae,
        "R¬≤": test_r2
    }
    
    print("--- TEST SET ---")
    print(f"RMSE : {test_rmse:.2f}")
    print(f"MAE  : {test_mae:.2f}")
    print(f"R¬≤   : {test_r2:.4f}\n")
    
    # Calcul du overfitting/underfitting
    rmse_diff = test_rmse - train_rmse
    r2_diff = train_r2 - test_r2
    
    metrics["analysis"] = {
        "RMSE_difference": rmse_diff,
        "R2_difference": r2_diff,
        "overfitting_status": "Possible overfitting" if r2_diff > 0.05 else "Normal"
    }
    
    return metrics


def print_metrics_summary(metrics):
    """
    Affiche un r√©sum√© format√© des m√©triques
    
    Args:
        metrics (dict): Dictionnaire contenant les m√©triques
    """
    if metrics is None:
        print("‚ùå Aucune m√©trique disponible")
        return
    
    print("\nM√©triques d'entra√Ænement:")
    for key, value in metrics["train"].items():
        print(f"  {key}: {value:.4f}")
    
    print("\nM√©triques de test:")
    for key, value in metrics["test"].items():
        print(f"  {key}: {value:.4f}")
    
    print("\nAnalyse:")
    print(f"  Diff√©rence RMSE (test - train): {metrics['analysis']['RMSE_difference']:.2f}")
    print(f"  Diff√©rence R¬≤ (train - test): {metrics['analysis']['R2_difference']:.4f}")
    print(f"  Status: {metrics['analysis']['overfitting_status']}")


if __name__ == "__main__":
    evaluate_model()
